# Regressziós technikák

<!--toc:start-->
- [Regressziós technikák](#regressziós-technikák)
  - [Regresszió](#regresszió)
    - [Lineáris regresszió](#lineáris-regresszió)
      - [Költségfüggvény](#költségfüggvény)
      - [Grádiens módszer](#grádiens-módszer)
      - [Bias](#bias)
      - [Grádiens módszer két paraméter esetén](#grádiens-módszer-két-paraméter-esetén)
  - [Források](#források)
<!--toc:end-->

## Regresszió
**Regresszió:** folytonos értékű címke (a címkehalmaz végtelen)

$$\vert Y \vert = \infty$$

**Példa:** autók számának, vagy életkor becslése.

### Lineáris regresszió
**Hipotézisfüggvény:** nagyon egyszerű (lineáris) hipotézisfüggvény:

$$y \approx \hat y = h_{\theta}(x) = \theta x$$

$\theta \in \R$ a hipotézisfüggvény paramétere, ebben az esetben az egyenes meredeksége lesz.

![](https://imgur.com/3251SJD.png)

#### Költségfüggvény
**Hogyan állapítjuk meg mennyire jó a becslés?**

A $J$ költségfüggvénnyel: $J : \theta \rightarrow \R_{\geq 0}$

A költségfüggvény megadja, hogy mennyire tér el a valódi címke és a
becslésünk adott paraméter értékek esetén.

Lineáris regresszió esetén a költségfüggvény:

$$J(\theta) = \frac{1}{2m}\sum_{j=1}^m{(h_{\theta}(x^{(j)}) - y^{(j)})^2}$$

Azaz, a hipotézisfüggvény által becsült címkék és az igazi címkék
különbségének a négyzete, átlagolva a mintaelemek felett:
**MSE (Mean Squared Error, átlagos négyzetes eltérés).**

**A feladat tehát:** $\theta^* = \mathrm{argmin}_{\theta}{J(\theta)}$

Keressük azt az optimális paramétert ($\theta^*$), mellyel minimális a költség,
azaz a címke predikciónk átlagos négyzetes eltérése az igazi címkétől.

$$\theta^* = \mathrm{argmin}_{\theta}{\frac{1}{2m}\sum_{j=1}^m{(h_{\theta}(x^{(j)}) - y^{(j)})^2}} = \\ \frac{1}{2m}\sum_{j=1}^m{(\theta x^{(j)} - y^{(j)})^2}$$

![](https://imgur.com/qFHnrpO.png)

#### Grádiens módszer
Honnan tudjuk, hogy merre kell lépnünk, hogy a költség csökkenjen?

Nézzük meg merre lejt leginkább a költségfüggvény az aktuális paraméterértékben állva!

Mi adja meg $J$ meredekségét egy adott $\theta^{\prime}$ pontban?
- a $J$ deriváltja a $\theta^{\prime}$ pontban.

$$J^{\prime}(\theta) = \frac{1}{2m}\sum_{j=1}^m{2 \cdot (\theta x^{(j)} - y^{(j)}) \cdot (\theta x^{(j)} - y^{(j)})^{\prime}} = \frac{1}{m}\sum_{j=1}^m{(\theta x^{(j)} - y^{(j)})x^{(j)}}$$

![](https://imgur.com/92Utvd6.png)

**Gradient descent:** Iteratívan lépegetünk $\theta$-val, abba az irányba, amerra
a legnagyobb a lejtése a költségfüggvénynek az aktuális $\theta$-ban.

```
repeat until convergence {
	grad := J'(θ)
	θ := θ - α * grad
}
```

$\alpha$: learning rate

#### Bias
**Mi hiányzik?**

Az eddigi modellünk túlzottan limitált. A hipotézisfüggvény egy olyan
egyenes volt, amely az origón kellett, hogy átmenjen $\dots$

A meredekség mellé egy konstans _(bias / intercept)_ paramétert is
bevezetünk.

**Új hipotézis:** $h(x) = \theta_1 x + \theta_0 = \hat y \approx y$

![](https://imgur.com/xvNKDPF.png)

A költségfüggvény továbbra is **MSE**, de a $h$ hipotézis függvény megváltozott.

A $J$ költségfüggvény továbbra is kvadratikus.
Mivel már két paraméterünk van, $J$ egy elliptikus paraboloid.

![](https://imgur.com/xDEcSK8.png)

**Hogyan válasszunk paramétereket, hogy a költség csökkenjen?**

Használjuk a gradiens módszert: lépegessünk a legnagyobb
meredekségű lejtés irányába.

Ezt az irányt egy adott pontban a gradiens vektor fogja megadni,
melynek elemei a J költségfüggvény parciális deriváltjai az egyes
paraméterek szerint.

**A költségfüggvény parciális deriváltjai:**

$$\frac{\partial}{\partial\theta_0} J(\theta_0, \theta_1) = \frac{1}{m}\sum_{j=1}^m{(\theta_1 x^{(j)} + \theta_0 - y^{(j)})}$$

$$\frac{\partial}{\partial\theta_1} J(\theta_0, \theta_1) = \frac{1}{m}\sum_{j=1}^m{(\theta_1 x^{(j)} + \theta_0 - y^{(j)}) \cdot x^{(j)}}$$

#### Grádiens módszer két paraméter esetén

```
repeat until convergence {
	grad0 := J'{θ0}(θ0, θ1)
	grad1 := J'{θ1}(θ0, θ1)
	
	θ0 := θ0 - α * grad0
	θ1 := θ1 - α * grad1	
}
```

{$\theta_n$}: $\theta_n$ szerinti parciális derivatív.


**Garantált-e, hogy a gradiens módszerrel megtaláljuk a lineáris regresszió optimális megoldását?**
- Megfelelően kicsi $\alpha$ esetén igen.

**Garantált-e, hogy a gradiens módszerrel megtaláljuk tetszőleges költségfüggvény globális minimumát?**
- **Nem.** Eljuthatunk az egyik lokális minimum pontba, de amennyiben a
költségfüggvény nem konvex, akkor nem garantált, hogy ez a globális
minimum lesz.

**Miért pont négyzetes hibát használunk költségként?**
- Ha analitikusan szeretnénk megoldani, akkor ez egyszerűbb
- Mivel a költségfüggvény kvadratikus, ezért a deriváltja lineáris, így a lépésméret "magától" leskálázódik, ahogy közelebb érünk az optimumhoz.

## Források
- [Diasor](https://drive.google.com/file/d/1NaaLk4C8kx1zUG2zA-9nUD-08abc7IzT/view)
- [Felvétel](https://nipgnas1.inf.elte.hu/~vavsaai@nipg.lab/annbsc23_p1_ea/annbsc23_p1_ea2.mp4)
 