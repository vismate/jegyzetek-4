# Többváltozós lineáris regresszió, klasszifikáció, logisztikus regresszió

<!--toc:start-->
- [Többváltozós lineáris regresszió, klasszifikáció, logisztikus regresszió](#többváltozós-lineáris-regresszió-klasszifikáció-logisztikus-regresszió)
  - [Több input-változós lineáris regresszió](#több-input-változós-lineáris-regresszió)
    - [Feature scaling](#feature-scaling)
  - [Klasszifikáció](#klasszifikáció)
    - [Bináris klasszifikáció](#bináris-klasszifikáció)
      - [Sigmoid függvény](#sigmoid-függvény)
  - [Logisztikus regresszió](#logisztikus-regresszió)
  - [Források](#források)
<!--toc:end-->

## Több input-változós lineáris regresszió

**Hipotézisfüggvény $n$ változó esetén:**
$$y \approx \hat y = h(x) = \theta_n x_n + \theta_{n-1} x_{n-1} + \dots + \theta_1 x_1 + \theta_0$$

**Vektoros alakban:**
$$y \approx \hat y = h(x^{(j)}) = \langle x^{(j)} \rangle = \sum_{i=0}^n{\theta_i x_i}$$

**Költségfüggvény $n$ változó esetén:**
$$J(\theta_0, \theta_1, \dots \theta_n) = \frac{1}{2m}\sum_{j=1}^m{(\theta_n x_n^{(j)} + \theta_{n-1} x_{n-1}^{(j)} + \dots + \theta_1 x_1^{(j)} - y^{(j)})^2}$$

### Feature scaling
Az input változók azonos nagyságrenre hozása

- $[0,1]$ intervallumra min-max skálázás
- sztenderdizálás **(ez preferált)**

$$\forall i,j : x_i^{(j)} = \frac{x_i^{(j)} - \min_j{(\{x_i^{(j)}\})}}{\max_j(\{x_i^{(j)}\}) - \min_j(\{x_i^{(j)}\})}$$

## Klasszifikáció
**Példa:** kategorizáljuk a fényképeket a rajta szereplő állatok szerint.

Pl. "kutya" = 1, "macska" = 2, "papagáj" = 3

Ekkor a regresszióval ez a baj: macska = 0.5 * "kutya" + 0.5 * "papagáj"

Ehelyett **valószínűséget fogunk becsülni**
- Mennyi a valószínűsége, hogy egy mintaelem az adott kategóriába tartozik?

### Bináris klasszifikáció
$$
\hat y = h(x) = P(x \ \mathrm{egy macska}) \\
\hat y = h(x) = 1 - P(x \ \mathrm{egy kutya})
$$

A becslés folytonos érték, és mivel valószínűséget becslünk, a címkék gyakorlatban folytonos értékek, így megpróbálkozhatunk lineáris regresszióval.

#### Sigmoid függvény
A lineáris regresszió nem ideális klasszifikációs feladatokhoz, mert nagyon érzékeny a kiugró értékekre.

Használhatunk inkébb **sigmoid** függvényt:
$$g(z) = \frac{1}{1 + e^{-z}}$$

**Logisztikus regresszió:**
$$h_{\theta}(x) = g(\theta_0 \theta_1 x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 x)}} = \hat y$$

## Logisztikus regresszió
Itt már a mean squared error költségfüggvény nem lesz konvex, ha a az új $h_{\theta}(x)$-et behelyettesítjük.

Itt inkább ezt használjuk (cross entropy):
$$J(\theta) = \frac{1}{m}\sum_{j=1}^m{[-y^{(j)}\log(h_{\theta}(x^{(j)})) - (1 - y^{(j)}) \log(1-h_{\theta}(x^{(j)}))]}$$

Ezután használhatjuk a gradiens módszert.

## Források
- [Diasor](https://canvas.elte.hu/courses/34761/modules/items/508995)
