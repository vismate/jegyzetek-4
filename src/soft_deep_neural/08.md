# 8. Előadás

<!--toc:start-->
- [8. Előadás](#8-előadás)
  - [Multilayer perceptron](#multilayer-perceptron)
    - [Kétrétegű neuronháló hipotézisfüggvénye](#kétrétegű-neuronháló-hipotézisfüggvénye)
    - [Aktivációs függvények](#aktivációs-függvények)
    - [Mit tanul egy neuronháló?](#mit-tanul-egy-neuronháló)
  - [Források](#források)
<!--toc:end-->

## Multilayer perceptron
- Az előző előadáson leírt neuronokat rétegekbe rendezzük, így egy komplexebb modell hozva létre.
- $\Theta = \{W_1, b_1, W_2, b_2, \dots\}$: súlymátrixok és bias vektorok halmaza.
  - $W_k \in \R^{S_k \times S_{k-1}}$
  - $b_k \in \R^{S_k}$
  - ahol $S_k$ a $k.$ rétegben található neuronok száma.

### Kétrétegű neuronháló hipotézisfüggvénye
$$h(x) = g_2(W_2 g_1(W_1x + b1) + b_2) = \hat y \approx y$$
- Költségfüggvények egyelőre maradnak.
  - Klasszifikáció: logistic loss
  - Regresszió: MSE
- A $g_1$ és $g_2$ aktivációs függvények fontosak, mert nemlinearitást adnak a hipotézisfüggvényhez, amely lehetővé teszi a tanulást. (különben nem nől a kifelyező erő, csak bonyolultabb lineáris regressziót kapunk)

### Aktivációs függvények
- sigmoid
- tanh
- ReLU

### Mit tanul egy neuronháló?

- Mit tanul egyetlen neuron?
  - Egyetlen lineáris döntési felületet (hiszen logisztikus regresszióról van szó)
- A neuronháló ezen lineáris döntési felületek valamilyen kombinációját tanulja

### Multiclass
- Ebben az esetben az $y$ nem egy skalár, hanem egy vektor.
- Multiclass klasszifikációnál használjuk a [softmax](TBA) függvényt az utolsó rétegen

## Források
- [Diasor](TBA)

